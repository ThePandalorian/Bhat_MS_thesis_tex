\epigraph{\justifying Somewhere [ ... ] between the specific that has no meaning and the general that has no content there must be, for each purpose and at each level of abstraction, an optimum degree of generality}{Kenneth Boulding}

\section{Background and motivation}

Raymond Pearl, one of the pioneers of mathematical ecology as a discipline, wrote in 1927 that ``What we want to know is how the biological forces of natality and mortality are so integrated and correlated in their action as to lead to a final result in size of population which follows this particular curve rather than some other one'' \citep{pearl_growth_1976}. Pearl realized 95 years ago that population dynamics must be ultimately explained by the mechanistic processes of birth and death. Today, there are mounting calls for more mechanistic models of evolution that are grounded in these fundamental processes of birth and death \citep{geritz_mathematical_2012,doebeli_towards_2017}. Ecologists also increasingly recognize that incorporating stochasticity is vital to developing more realistic ecological models \citep{hastings_transients_2004, coulson_skeletons_2004, boettiger_noise_2018, shoemaker_integrating_2020,schreiber_does_2022} and does more than `add noise' to deterministic expectations. Individual-based models, where ecological rules are specified at the level of the individual, are a powerful mathematical tool for mechanistic descriptions of stochastic population dynamics \citep{black_stochastic_2012}. Birth-death processes are a very general class of stochastic processes that can be used to capture a wide range of eco-evolutionary processes. `System-size expansions' and their subsequent analysis using the `weak noise approximation' are common tools for analyzing stochastic birth-death processes that are well-known in the statistical physics and applied mathematics communities \citep{gardiner_stochastic_2009}. However, these tools remain relatively underappreciated in ecology, despite being relatively easy to understand and extremely well-motivated in scenarios germane to ecology and evolution. Here, I present a formulation of population dynamics constructed from first principles grounded in birth-death processes. To facilitate readership by a broad audience, only passing familiarity with calculus (derivatives, integrals, Taylor expansions) and probability are assumed. Familiarity with stochastic calculus is helpful for some sections but is not required, and I provide a brief introduction below. I begin by introducing birth-death processes, SDEs, and the Fokker-Planck equation in a non-technical manner, and providing the intuition for system-size expansions and weak noise analysis in ecological systems. In section \ref{sec_1D_processes}, I show how fluctuating population size of populations of identical individuals can be tracked through a one-dimensional birth-death process. I introduce a description of the system via a `master equation', and then conduct a `system-size expansion' to obtain a Fokker-Planck equation for the system. Finally, I conduct a weak noise approximation to arrive at a linear Fokker-Planck equation which can be solved exactly using some stochastic calculus to arrive at a closed-form solution given by a time-dependent Ornstein-Uhlenbeck process. As an example, I illustrate the complete process for a stochastic version of the logistic equation. In section \ref{sec_nD_processes}, I present a multivariate process to describe the evolution of discretely varying traits, and, as before, illustrate the system size expansion and the weak noise approximation. Under mild assumptions, we show that the deterministic limit of this process is the well-known replicator-mutator equation (or equivalently, Eigen's quasispecies equation), thus establishing the microscopic basis of well-known equations in evolutionary game theory. I also show that the mean value of the trait in the population changes according to the Price equation in the deterministic limit. Chapter \ref{chap_infD_processes} introduces a function-valued process to model the evolution of quantitative traits such as body size, which can take on uncountably many values. This function-valued process can then also be analyzed via a system-size approximation to arrive at a `functional' Fokker-Planck equation, in terms of functional derivatives. Under mild assumptions, we show that classic equations such as Kimura's infinite alleles model and the canonical equation of adaptive dynamics can be derived as the deterministic limits of this stochastic process. We also conduct a weak noise approximation to arrive at a linear functional Fokker-Planck equation.  Sections \ref{sec_1D_processes} and \ref{sec_nD_processes} apply techniques that are well-known in physics, as applied to population biology. In contrast, chapter \ref{chap_infD_processes}, to the best of my knowledge, is entirely original. Chapter \ref{chap_infD_processes} also presents a simple heuristic derivation of quantitative genetics models and adaptive dynamics from stochastic first principles that is much simpler than the rigorous mathematical derivations grounded in measure theory and martingale/markov theory that are currently standard reference in theoretical ecology \citep{champagnat_individual_2008}. I illustrate the utility of all this abstract formalism through examples in Chapter \ref{chap_examples}.

\subsection{Birth-death processes}
Mathematically, a birth-death process is a so-called `continuous-time Markov chain' in which only transitions between local states are allowed. In other words, a birth-death process is a stochastic process unfolding in continuous time such that
\begin{itemize}
	\item The process is `Markov', meaning that the future is statistically independent of the past given the present. In more mathematical terms, if the value of the stochastic process at time $t$ is given by $X_t$, $\mathbb{P}(\cdot | E)$ denotes probability conditioned on $E$, and $u < s \leq t$, then
	\begin{equation*}
	\mathbb{P}(X_t | X_s, X_u) = \mathbb{P}(X_t | X_s)
	\end{equation*}
	\item Direct transitions must be `local'. Mathematicians usually reserve the phrase `birth-death process' to processes that take values in the non-negative integers $\{0,1,2,3,4,\ldots\}$. In this case, only direct transitions from $n$ to $n \pm 1$ are allowed to occur. Biologically, this is saying that we observe the population on a fine enough timescale that the probability of two or more births/deaths occurring at the exact same time is very low and we can disallow it entirely in our models. The conditions for higher dimensional birth-death processes look similar.
\end{itemize}
Since these processes unfold in continuous time, they are characterized not by transition probabilities but by transition \emph{rates}, which can be thought of as the probability of transition `per unit time'. The quantity of interest is usually the probability of being in a particular state at a given point in time. The entire birth-death process can be described in terms of such a quantity, through a so-called `Master equation'. The master equation is a partial differential equation (PDE) for the probability of being in a given state at a given time, However, in all but the simplest cases, we can't actually solve this PDE, because it is simply too hard. The primary source of difficulty is non-linearity in the transition rates and the fact that transitions occur in discrete, discontinuous `jumps'. It is much easier to describe and analyze systems by using tools from stochastic calculus and partial differential equations, as we describe below.

\subsection{SDEs and the Fokker-Planck equation}\label{intro_SDE}
Stochastic systems which change continuously (in the state space) can be described in terms of a `stochastic differential equation' (SDE), which here is interchangeable with the phrase `It\^o process'. An SDE for a stochastic process $\{X_t\}_{t \geq 0}$ is an equation of the form
\begin{equation}
\label{ito_SDE_integral}
X_t = \int\limits_{0}^{t} F(s,X_s)ds + \int\limits_{0}^{t} G(s,X_s)dB_s
\end{equation}
where $F(t,x)$ and $G(t,x)$ are `nice' functions\footnote{For the mathematically oriented reader, there are two requirements: Firstly, we require the functions to have `linear growth', meaning that we can find a constant $C > 0$ such that $\|F(t,x)\| + \|G(t,x)\| \leq C(1+\|x\|)$ for every $x \in \mathbb{R}^{d}$ and $t > 0$. We also require `Lipschitz continuity', which means that we can find a constant $L > 0$ such that $\|F(t,x) - F(t,y)\| + \|G(t,x)-G(t,y)\| \leq L\|x-y\|$ for every pair $x,y \in \mathbb{R}^d$ and $t > 0$. Here, $\|\cdot\|$ denotes the natural norm on the space under consideration and for our cases will usually be the Euclidean norm. For biological systems, both of these conditions will usually be satisfied, and so we assume going further that all our SDEs are always well-defined and admit solutions.} In the physics literature, $F$ and $G$ are often called the `drift' and `diffusion' of the process respectively. However, we will not use this terminology here due to  potential confusion with genetic drift (which actually corresponds to the `diffusion' in the physics terminology). $B_t$ is the so-called `standard Brownian motion'. Named after the botanist Robert Brown (who was looking at the random erratic motion of pollen grains in water under a microscope), $\{B_t\}_{t \geq 0}$ is a stochastic process that is supposed to model `random noise' or `undirected diffusion' of a particle. If one imagines $B_t$ as recording the position of a small pollen grain at time $t$, then $B_t$ can be formally thought of as a process that has the following properties:
\begin{itemize}
	\item It starts at the origin, \emph{i.e} $B_0 = 0$. This is a harmless assumption made for convenience and amounts to a choice of coordinate system.
	\item It moves continuously, without sudden jumps across regions of space, \emph{i.e} the map $t \to B_t$ is continuous. This simply says that our pollen grain moves short distances in short intervals of time.
	\item The future movement is independent of past history. That is, given times $u < s < t$, the displacement $B_t - B_s$ is independent of the past position $B_u$.
	\item The movement is directionless and random, and displacement is normally distributed. More precisely, given two times $s < t$, the displacement $B_t - B_s$ follows a normal distribution with a mean of $0$ (this is the `directionless' part) and a variance of $t-s$ (this is the `random' part).
\end{itemize}
It can then be shown that since the motion is equally likely to be in any direction, the expected position at any point of time is the same as the initial position, \emph{i.e} $\mathbb{E}[B_t | B_0] = B_0 = 0$.\\
The second integral in equation \eqref{ito_SDE_integral} is It\^o's `stochastic integral', and is to be interpreted in the following sense: Fix a time $T > 0$. Partition the interval $[0,T]$ into $n$ intervals of the form $[t_i,t_{i+1}]$ such that $0 = t_0 < t_1 < t_2 < \ldots < t_n = T$. Then, the (It\^o) stochastic integral from $0$ to $T$ can be thought of as:
\begin{equation*}
\int\limits_{0}^{T} G(s,X_s)dB_s \coloneqq \lim_{n \to \infty} \sum\limits_{i=1}^{n}G(t_i,X_{t_i})(B_{t_{i+1}}-B_{t_i})  
\end{equation*}
That is to say, it is obtained by making successively finer partitions of the form $[t_i,t_{i+1}]$, and then computing the `area of the rectangle' formed with $B_{t_{i+1}}-B_{t_i}$ and $G(t_i,X_{t_i})$ as sides. This should look similar to the classic Riemann integral, with the uniform width $t_{i+1}-t_i$ of the Riemann integral replaced by a random width corresponding to the random displacement of a Brownian particle during the uniform time interval $[t_i,t_{i+1}]$.\\
\\
Equation \eqref{ito_SDE_integral} is often represented in the compact form:
\begin{equation}
\label{ito_SDE_diff}
dX_t = F(t,X_t)dt + G(t,X_t)dB_t
\end{equation}
. The physics literature also often uses the `Langevin form':
\begin{equation}
\label{ito_langevin}
\frac{dx}{dt} = F(t,x) + G(t,x)\eta(t)
\end{equation}
where $\eta(t)$ is supposed to be `Gaussian white noise', defined indirectly such that $\int_0^{t}G(s,x)\eta(s)ds$ behaves the same as $\int_0^{t}G(s,X_s)dB_s$. However, it is important to remember that these are both purely formal expressions - Equation \eqref{ito_SDE_diff} is meaningless on its own and is really just shorthand for equation \eqref{ito_SDE_integral}, which is well-defined as explained above; Equation \eqref{ito_langevin} is even worse, because the Brownian motion is known to be non-differentiable, and as such, $\eta(t)$ cannot really exist - Both equations are thus to be interpreted as shorthand for equation \eqref{ito_SDE_integral}, which formally `makes sense'. SDEs are convenient because they satisfy several `nice' analytical properties. For example, using the fact that the Brownian motion has no expected change in value (\emph{i.e} $\mathbb{E}[B_t | B_0] = B_0 = 0$), it can be shown\footnote{We can actually prove something stronger: We can show under rather mild regularity assumptions on $X_t$ and $G(t,x)$ that the stochastic integral is a continuous square-integrable martingale starting at the origin - This means that the map $t \to \int_0^{t}G(s,X_s)dB_s$ is continuous, starts at the origin, and always has an expectation value of $0$.} that the stochastic integral also has an expectation value of $0$ for all $t$, \emph{i.e}:
\begin{equation*}
\mathbb{E}\left[\int\limits_{0}^{t} G(s,X_s)dB_s \bigg{|} X_0\right] = 0
\end{equation*}
Using this, and the fact that the future path of the Brownian motion itself is independent of its history, one can derive the following `notational algebra table' for manipulating products of formal expressions of the form \eqref{ito_SDE_diff}:
\\
\begin{center}
	\begin{tabularx}{0.4\textwidth}{ 
			| >{\centering\arraybackslash}X 
			| >{\centering\arraybackslash}X 
			| >{\centering\arraybackslash}X | }
		\hline
		& $\mathbf{dt}$ & $\mathbf{dB_t}$ \\
		\hline
		$\mathbf{dt}$ & $0$  &  $0$ \\ 
		\hline
		$\mathbf{dB_t}$ & $0$  & $dt$ \\
		\hline
	\end{tabularx}
\end{center}
which becomes very useful for formal manipulation. Using this property, one can show using some simple algebra that if a process $X_t$ taking values in $\mathbb{R}$ satisfies the SDE \eqref{ito_SDE_diff}, then the probability density $P(x,t)$ of finding the process in a state $x \in \mathbb{R}$ at time $t$ satisfies the PDE
\begin{equation}
\label{ito_FPE}
\frac{\partial P}{\partial t}(x,t) = -\frac{\partial}{\partial x}\{F(t,x)P(x,t)\} + \frac{1}{2}\frac{\partial^2}{\partial x^2}\{(G(t,x))^2P(x,t)\}
\end{equation}
. I present a simple informal derivation in Appendix \ref{App_SDE_FPE}. Equation \eqref{ito_FPE} is called the `Fokker-Planck equation' in the physics and applied mathematics literature \citep{gardiner_stochastic_2009} and is often called the `Kolmogorov forward equation' in the population genetics \citep{ewens_mathematical_2004} and pure mathematics \citep{oksendal_stochastic_1998} literature. If the function $G$ is independent of $x$, then it comes out of the derivatives in equation \eqref{ito_FPE}, and the resultant Fokker-Planck equation is said to be `linear' (and is much easier to solve). This link between SDEs and Fokker-Planck equations goes both ways: One can show that every stochastic process with a probability density described by a Fokker-Planck equation of the form \eqref{ito_FPE} corresponds to the solution of an SDE of the form \eqref{ito_SDE_diff}, though the proof is much more technical and will not be discussed here. This two-way correspondence proves to be extremely useful, as one approach often works for applications in which the other fails. This correspondence makes it greatly desirable to be able to describe our stochastic process of interest as either the solution to an It\^o SDE of the form \eqref{ito_SDE_diff} or as the solution to a Fokker-Planck equation of the form \eqref{ito_FPE}. System-size expansions facilitate such a description for birth-death processes.

\subsection{Density-dependence and the intuition for system-size expansions in ecology}
The fundamental idea behind the system-size expansion relates to the nature of the jumps between successive states of a birth-death process. In most situations in ecology, at an individual level, births and deaths of individuals are affected by local population density and not directly by the total population size. Despite this, the jumps themselves occur in terms of the addition (birth) or removal (death) of a \emph{single individual} from the population. If there are many individuals, each individual contributes a negligible amount to the density, and thus, the discontinuous jumps due to individual-level births or deaths can look like a small, \emph{continuous} change in population density. This is the essential idea behind the system-size expansion. The name derives from the formalization of this idea as a change of variable from the discrete values $\{0,1,2,\ldots,n-1,n,n+1,\ldots\}$ to the approximately continuous values $\{0,1/K,2/K,\dots,x-1/K,x,x+1/K,\ldots\}$ by the introduction of a `system size parameter' $K$. In ecology, this parameter will be some fundamental limit on resources, such as habitat size or carrying capacity. In physics and chemistry, it is usually the total volume of a container in which physical or chemical reactions take place. When $K$ is large, the fact that transitions occur in units of a small value $1/K$ can be exploited via a Taylor expansion of the transition rates in the Master equation, which then yields a Fokker-Planck equation upon neglecting higher order terms. A similar approximation is well-known (ever since Fisher) in theoretical population genetics \citep{ewens_mathematical_2004}, where it goes by the name of the `diffusion approximation', and has been heavily used by Kimura \citep{crow_introduction_1970} in his stochastic models. However, the population genetics version of the approximation often lacks an explicit system size parameter (in physics parlance \citep{gardiner_stochastic_2009}, it is closer to a Kramers-Moyal expansion than a Van Kampen expansion) and is thus often somewhat ad-hoc.


\subsection{The intuition for the weak noise approximation in ecology}
If the parameter $K$ is sufficiently large, then the Fokker-Planck equation obtained via the system-size expansion can be further simplified to obtain a linear Fokker-Planck equation. This is accomplished by viewing the stochastic dynamics as fluctuating about a deterministic trajectory (obtained by letting $K \to \infty$) and only works if $K$ is large enough to be able to neglect all but the highest-order terms. This is usually an excellent approximation for populations in which the deterministic trajectory has already reached an attractor (stable fixed point, stable limit cycle, etc.). Since many deterministic eco-evolutionary models are expected to relax to such attractors, such an approximation is a useful first step in increasing the generality of existing models (which are usually studied only in the equilibrium regime) to incorporate the dynamics of finite populations. Importantly, this approximation \emph{only} works if we can discard all but highest-order terms of $K$: Including higher-order terms leads to equations that do not form Fokker-Planck equations and do not even describe probability densities. As such, this approximation is best suited to describe populations that are `medium sized' - small enough that they cannot be assumed to be infinitely large, yet large enough that stochasticity is rather weak and the deterministic limit is somewhat predictive - A situation that occurs frequently in ecology and evolution.

\section{Warm up: One-dimensional processes for population size}\label{sec_1D_processes}
The simplest birth-death processes are those in which the state at any time can be characterized by a single number. Populations of identical individuals are an obvious example of such a system. The mathematics below are adapted from sections 6.3 and 7.2 of \citep{gardiner_stochastic_2009}.

\subsection{Description of the process and the Master Equation}

Consider a population of identical individuals subject to some ecological rules that affect individuals' birth and death rates. Since all individuals are identical, we can only really track the population size through time. The population as a whole at any time $t$ can thus be characterized by a single number - its population size (Figure \ref{fig_1D_pop_description}). Imagine further that if a population has $n$ identical individuals, then, from the ecological rules, we can determine a \emph{birth rate} $b(n)$, which gives us a measure of the probability that a new individual will be born and the population size becomes $n+1$ `per unit time'. One must be slightly precise about what exactly they mean when they say `per unit time' since there are no discrete `time steps' for individuals to be born. Here, by `birth rate', we mean the probability that there will be a birth (and no death) per an \emph{infinitesimal} amount of time. More formally, letting $N_t$ denote the random variable representing the population size at time $t$ and letting $\mathbb{P}(E)$ denote the probability (in the common-sense usage) of an event $E$, the birth rate $b(n)$ of a population with population size $n$ is the quantity
\begin{equation}
\label{1D_birthrate_defn}
b(n) \coloneqq \lim_{\epsilon \to 0}\frac{1}{\epsilon}\mathbb{P}\left(N_{t+\epsilon}=n+1 | N_{t}=n\right)
\end{equation}
\\
Exactly analogously, we also assume we can define a \emph{death rate} $d(n)$ of a population of $n$ individuals as the quantity
\begin{equation}
\label{1D_deathrate_defn}
d(n) \coloneqq \lim_{\epsilon \to 0}\frac{1}{\epsilon}\mathbb{P}\left(N_{t+\epsilon}=n-1 | N_{t}=n\right)
\end{equation}
An alternative, perhaps more intuitive characterization, of these same quantities is the following: If we have a population of size $n$, and we know that \emph{either a birth or a death} has just occurred, then, the probability that the event that occurred is a birth is
\begin{equation*}
    \mathbb{P[\textrm{ birth } | \textrm{ something happened }]} = \frac{b(n)}{b(n)+d(n)}
\end{equation*}
and the probability that the event was instead a death is given by
\begin{equation*}
    \mathbb{P[\textrm{ death } | \textrm{ something happened }]} = \frac{d(n)}{b(n)+d(n)}
\end{equation*}
\\
\begin{example}\label{ex_1D_stoch_logistic}
Consider the case where the per-capita birth rate is a constant $\lambda > 0$, \emph{i.e}, $b(n) = \lambda n$, and the per-capita death rate has the linear density-dependence $d(n) = \left(\mu + (\lambda-\mu)\frac{n}{K}\right)n$, where $\mu$ and $K$ are positive constants. Taking the difference between the birth and death rates, we obtain $b(n) - d(n) = (\lambda - \mu)n\left(1-\frac{n}{K}\right)$, where, identifying $r=\lambda-\mu$, we obtain the familiar logistic equation on the RHS. Note, however, that the population itself is stochastic, whereas the logistic equation is a deterministic description.
\end{example}
Now, let $P(n,t)$ be the probability that the population size is $n$ at time $t$. We wish to have an equation to describe how $P(n,t)$ changes with time - this will provide a probabilistic description of how we expect the population size to change over time.
\myfig{0.5}{Media/3.1_BD_process_1D.png}{\textbf{Schematic description of a one-dimensional birth-death process}. Consider a population of identical individuals. The state of the system can be described by a single number, in this case, the population size. Births and deaths result in changes in the total population size, and the birth and death rates are dependent on the current population size.}{fig_1D_pop_description}

To do this, we imagine a large ensemble of populations. In a large ensemble of copies evolving independently, a fraction $P(n,t)$ will have population size $n$ at time $t$ by definition of probability. We can now simply measure the `inflow' and `outflow' of copies of the population from each state. If a population has $n$ individuals, it could either have gotten there from a population of $n+1$ individuals, with a death rate of $d(n+1)$, or from a population of $n-1$ individuals, with a birth rate of $b(n-1)$. Thus, the rate of `inflow' to the state $n$ is given by
\begin{equation}
    \label{1D_rate_in}
    R_{\textrm{in}}(n,t) = b(n-1)P(n-1,t) + d(n+1)P(n+1,t)
\end{equation}
Similarly, if the population has $n$ individuals, it could obtain a different state in two ways: With rate $b(n)$, the population witnesses a birth, and with rate $d(n)$, it witnesses a death. Thus, the rate of `outflow' is given by
\begin{equation}
    \label{1D_rate_out}
    R_{\textrm{out}}(n,t) = b(n)P(n,t) + d(n)P(n,t)
\end{equation}
The rate of change of the probability of the system being in state $n$ is given by the rate of inflow minus the rate of outflow. Thus, we have
\begin{align}
    \frac{\partial P}{\partial t}(n,t) &= R_{\textrm{in}}(n,t) - R_{\textrm{out}}(n,t)\nonumber\\
    &= b(n-1)P(n-1,t) + d(n+1)P(n+1,t) - b(n)P(n,t) - d(n)P(n,t)\label{1D_M_eqn_nostep}
\end{align}
For convenience, let us define two `step operators' $\mathcal{E}^{\pm}$, which act on any functions of populations to their right by either adding or removing an individual, \textit{i.e}
\begin{equation*}
    \mathcal{E}^{\pm}f(n,t) = f(n \pm 1,t)
\end{equation*}
Rearranging the RHS of \eqref{1D_M_eqn_nostep} to write in terms of these step operators, we obtain the compact expression
\begin{equation}
\label{1D_M_eqn}
\frac{\partial P}{\partial t}(n,t) = (\mathcal{E}^{-}-1)b(n)P(n,t) + (\mathcal{E}^{+}-1)d(n)P(n,t)
\end{equation}
This is the so-called `master equation', and completely describes our system. However, in general, $b(n)$ and $d(n)$ may be rather complicated, in which case it may not be possible to solve \eqref{1D_M_eqn} directly.

\subsection{The system-size expansion}
The system-size expansion arises from noting that in many systems, the interactions are governed not by population size, but by population \emph{density}. However, the population jumps themselves are discretized at the scale of the individual, which becomes negligibly small if we have a large population density. Thus, we assume that there exists a system-size parameter $K > 0$ such that the discrete jumps between states happen in units of $1/K$, and we make the substitutions
\begin{align*}
    x &= \frac{n}{K}\\
    b_K(x) &= \frac{1}{K}b(n)\\
    d_K(x) &= \frac{1}{K}d(n)
\end{align*}
As $K$ grows very large, the discontinuous jumps in $n$ thus appear like `continuous' transitions in our new variable $x$, which can be thought of as the `density' of organisms. A system-size parameter $K$ often naturally emerges in ecological systems through resource-limiting factors such as habitat size or carrying capacity. Under these substitutions, equation \eqref{1D_M_eqn} becomes
\begin{equation}
\label{1D_M_eqn_density}
\frac{\partial P}{\partial t}(x,t) = (\Delta^{-}-1)Kb_K(x)P(x,t) + (\Delta^{+}-1)Kd_K(x)P(x,t)
\end{equation}
where we now have the new step operators
\begin{equation}
\label{1D_step_operators_density}
\Delta^{\pm}f(x,t) = f\left(x\pm\frac{1}{K},t\right) 
\end{equation}
If $K$ is large, then we can now taylor-expand the action of these step operators as:
\begin{equation*}
\Delta^{\pm}f(x,t) = f\left(x\pm\frac{1}{K},t\right) = f(x,t) \pm \frac{1}{K}\frac{\partial f}{\partial x}(x,t) + \frac{1}{2K^2}\frac{\partial^2f}{\partial x^2}(x,t) + \mathcal{O}(K^{-3})
\end{equation*}
Substituting these expansions into \eqref{1D_M_eqn_density} and neglecting terms of $\mathcal{O}(K^{-3})$ and higher, we obtain
\begin{equation}
\label{1D_FPE}
\setlength{\fboxsep}{2\fboxsep}\boxed{
\frac{\partial P}{\partial t}(x,t) = -\frac{ \partial}{\partial x}\{A^{-}(x)P(x,t)\} + \frac{1}{2K}\frac{\partial^2}{\partial x^2}\{A^{+}(x)P(x,t)\}
}
\end{equation}
where
\begin{equation*}
A^{\pm}(x) = b_K(x) \pm d_K(x)
\end{equation*}
Equation \eqref{1D_FPE} has the form of a so-called `Fokker-Planck equation', and corresponds to the SDE:
\begin{equation}
\label{1D_SDE}
dX_t = A^{-}(X_t)dt + \sqrt{\frac{A^{+}(X_t)}{K}}dB_t
\end{equation}
interpreted in the It\^{o} sense. Note that the deterministic component of this process depends on the difference between birth and death rates (a mechanistic measure of Malthusian fitness), whereas the stochastic part depends on their sum and scales inversely with $\sqrt{K}$.

\subsection{Stochastic fluctuations and the weak noise approximation}\label{sec_1D_WNA}
% The mathematics in this section is taken from \cite{gardiner_stochastic_2009} with minor modifications\footnote{Gardiner uses a parameter $\epsilon>0$ such that noise increases with an increase in $\epsilon$. I have used a parameter $K>0$ such that noise decreases with an increase in $K$, which is more biologically relevant. Gardiner's exact results can be recovered by substituting $\epsilon = \frac{1}{\sqrt{K}}$}.
If we assume the noise is \emph{weak}, then we can go still further with analytic techniques by measuring fluctuations from the deterministic expectations, albeit with some slightly cumbersome calculations to arrive at the final expressions. We will grit our teeth and get through the algebra below, with my promise that the final answer is neat and easy to handle. It is clear that as $K \to \infty$, equation \eqref{1D_SDE} describes a deterministic process, obtained as the solution to
\begin{equation}
\label{1D_det_limit}
    \frac{dx}{dt} = A^{-}(x) = b_{K}(x) - d_{K}(x)
\end{equation}
This is a very intuitive equation, saying that the rate of change of the population is equal to the birth rate minus the death rate. Let the solution of this equation  be given by $\alpha(t)$, so that $\frac{d{\alpha}}{dt}(t) = A^{-}(\alpha(t))$.\\
We can now measure (scaled) fluctuations from the deterministic solution $\alpha$ through a new variable $y=\sqrt{K}\left(x-\alpha(t)\right)$. For notational clarity, we will also introduce a new time variable $s=t$ which is equal to the original time variable (this is just so the equations look clearer). Let the probability density function of this new variable be given by $\Tilde{P}(y,s)$. In summary, we have introduced the variables:
\begin{align*}
    y &= \sqrt{K}\left(x-\alpha(t)\right)\\
    s &= t\\
    \Tilde{P}(y,s) &= \frac{1}{\sqrt{K}}P(x,t)
\end{align*}
Note that by ordinary rules of variable substitution, we have:
\begin{align}
    \frac{\partial \Tilde{P}}{\partial t} &= \frac{\partial \Tilde{P}}{\partial y}\frac{\partial y}{\partial t} + \frac{\partial \Tilde{P}}{\partial s}\frac{\partial s}{\partial t}\nonumber\\
    &=\frac{\partial \Tilde{P}}{\partial y}\left( -\sqrt{K}\frac{d\alpha}{dt}\right) + \frac{\partial \Tilde{P}}{\partial s}\nonumber\\
    &= -\sqrt{K}A^{-}(\alpha(s))\frac{\partial \Tilde{P}}{\partial y} + \frac{\partial \Tilde{P}}{\partial s}\label{weak_noise_expansion_first_term}
\end{align}
and
\begin{equation}
\label{weak_noise_expansion_second_term}
    \frac{\partial }{\partial y} = \frac{1}{\sqrt{K}}\frac{\partial }{\partial x}
\end{equation}
Reformulating \eqref{1D_FPE} in terms of $y,s$ and $\Tilde{P}$ and substituting \eqref{weak_noise_expansion_first_term} and \eqref{weak_noise_expansion_second_term} yields:
\begin{align}
-A^-(\alpha)\frac{\partial\Tilde{P}}{\partial x} + \frac{\partial \Tilde{P}}{\partial s} &= -\sqrt{K}\frac{\partial}{\partial y}\left(A^-(\alpha+\frac{y}{\sqrt{K}})\Tilde{P}\right)+\frac{1}{2}\frac{\partial^2}{\partial y^2}\left(A^+(\alpha+\frac{y}{\sqrt{K}})\Tilde{P}\right)\nonumber\\
\Rightarrow \frac{\partial \Tilde{P}}{\partial s} &= -\frac{\partial}{\partial y}\left[\sqrt{K}\left(A^-(\alpha+\frac{y}{\sqrt{K}})-A^-(\alpha)\right)\Tilde{P}\right]+\frac{1}{2}\frac{\partial^2}{\partial y^2}\left(A^+(\alpha+\frac{y}{\sqrt{K}})\Tilde{P}\right)\label{weak_noise_exact_equation}
\end{align}
We are now ready to make a weak noise `expansion'. We do so by assuming that $\Tilde{P}$, $A^-(\alpha + \frac{y}{\sqrt{K}})$, and $A^+(\alpha+\frac{y}{\sqrt{K}})$ can be approximated by series expansions in $\frac{1}{\sqrt{K}}$ of the form:
\begin{align*}
    \Tilde{P} &= \sum\limits_{n=0}^{\infty}\Tilde{P}_n\left(\frac{1}{\sqrt{K}}\right)^n\\
    A^-\left(\alpha(s) + \frac{y}{\sqrt{K}}\right) &= \sum\limits_{n=0}^{\infty}A^-_n(s)\left(\frac{y}{\sqrt{K}}\right)^n\\
    A^+\left(\alpha(s) + \frac{y}{\sqrt{K}}\right) &= \sum\limits_{n=0}^{\infty}A^+_n(s)\left(\frac{y}{\sqrt{K}}\right)^n
\end{align*}
with $A^-_0(s) = A^-(\alpha(s)), A^+_0(s) = A^+(\alpha(s))$. These could be Taylor expansions, for example, but the exact form of the coefficients is irrelevant as long as it is known to us, so any expansion will work. We can now substitute these series expansions into \eqref{weak_noise_exact_equation} to obtain:
\begin{equation}
\begin{split}
\label{weak_noise_full_series_expansion}
\sum\limits_{n=0}^{\infty}\left(\frac{1}{\sqrt{K}}\right)^n\frac{\partial \Tilde{P}_n}{\partial s} = 
-\frac{\partial}{\partial y}\left[\sqrt{K}\left( \sum\limits_{n=1}^{\infty}A^-_n(s)\left(\frac{y}{\sqrt{K}}\right)^n\right)\left(\sum\limits_{m=0}^{\infty}\Tilde{P}_m\left(\frac{1}{\sqrt{K}}\right)^m\right)\right] \\ + 
\frac{1}{2}\frac{\partial^2}{\partial y^2}\left[\left(\sum\limits_{n=0}^{\infty}A^+_n(s)\left(\frac{y}{\sqrt{K}}\right)^n\right)\left(\sum\limits_{m=0}^{\infty}\Tilde{P}_m\left(\frac{1}{\sqrt{K}}\right)^m\right)\right]
\end{split}
\end{equation}
We can now compare the coefficients of $K^{-n/2}$ for each $n$ in order to arrive at approximations in the series expansion, the idea being that you neglect all terms which are of order greater than $\mathcal{O}(K^{-m/2})$ for some $m$ according to the desired precision.\\
We observe that for any fixed $r$, the coefficient of $K^{-r/2}$ on the LHS is $\frac{\partial \Tilde{P}_r}{\partial s}$. On the RHS, the coefficients of $K^{-r/2}$ in the second term have the form $\Tilde{P}_{m}A^+_{n}y^n$, subject to the constraint that $m+n=r$. Furthermore, all such terms (and only such terms) are coefficients of $K^{-r/2}$. Thus, after grouping, the coefficient of $K^{-r/2}$ from the second terms of the RHS of \eqref{weak_noise_full_series_expansion} is precisely
\begin{equation*}
    \frac{1}{2}\frac{\partial^2}{\partial y^2}\sum\limits_{m=0}^{r}\Tilde{P}_{m}A^+_{r-m}y^{r-m}
\end{equation*}
Exactly analogous reasoning reveals that the contribution of the first term of the RHS is:
\begin{equation*}
    -\frac{\partial}{\partial y}\sum\limits_{m=0}^{r}\Tilde{P}_{m}A^-_{r-m+1}y^{r-m+1}
\end{equation*}
Thus, we find that the $r$th term of the expansion satisfies:
\begin{equation}
\label{weak_noise_expansion_each_term}
\frac{\partial \Tilde{P}_r}{\partial s} = -\frac{\partial}{\partial y}\left(\sum\limits_{m=0}^{r}\Tilde{P}_{m}A^-_{r-m+1}y^{r-m+1}\right) + \frac{1}{2}\frac{\partial^2}{\partial y^2}\left(\sum\limits_{m=0}^{r}\Tilde{P}_{m}A^+_{r-m}y^{r-m}\right)
\end{equation}
If we assume we can obtain a reasonable approximation by retaining only the first term of the expansion and neglecting all higher-order terms\footnote{For example, if the deterministic trajectory is at a stable fixed point and subject to weak fluctuations}, we are left with the expression:
\begin{equation}
\label{1D_WNA}
\frac{\partial \Tilde{P}_0}{\partial s} = -A^-_1(s)\frac{\partial}{\partial y}(y\Tilde{P}_0) + \frac{A^+_{0}(s)}{2}\frac{\partial^2 \Tilde{P}_{0}}{\partial y^2}
\end{equation}
which is simply the Fokker-Planck equation for the It\^{o} process
\begin{equation*}
    dY_t = A^-_1(t)Y_tdt + \sqrt{A^+_0(t)}dB_t
\end{equation*}
This equation describes a so-called `Ornstein-Uhlenbeck process', and is easily solved by using $\exp(-\int A^-_1(s)ds)$ as an `integrating factor'. In particular, multiplying both sides by $\exp(-\int A^-_1(s)ds)$ yields
\begin{align*}
    \exp\left(-{\int\limits_{0}^{t}A^-_1(s)ds}\right)dY_t - Y_tA^-_1(t)\exp\left(-{\int\limits_{0}^{t}A^-_1(s)ds}\right)dt &= \sqrt{A^+_0(t)}\exp\left(-{\int\limits_{0}^{t}A^-_1(s)ds}\right)dB_t\\
    \Rightarrow d\left(\exp\left(-{\int\limits_{0}^{t}A^-_1(s)ds}\right)Y_t\right) &= \sqrt{A^+_0(t)}\exp\left(-{\int\limits_{0}^{t}A^-_1(s)ds}\right)dB_t
\end{align*}
Integrating both sides and noting that $A^+_0(s) = A^+(\alpha(s))$, we thus obtain the final expression
\begin{equation}
\label{weak_noise_OU_solution}
    Y_t = Y_0\exp\left({\int\limits_{0}^{t}A^-_1(s)ds}\right)+\int\limits_{0}^{t}\exp\left(-\int\limits_{s}^{t}A^-_{1}(v)dv\right)\sqrt{A^+(\alpha(s))}dB_s
\end{equation}
as the zeroth-order weak noise approximation for stochastic fluctuations from the deterministic trajectory due to demographic noise Note that this is an exact equation, and one can get many insights from it. For example, if $Y_0 = 0$ (\emph{i.e} we start at the deterministic steady state, a natural assumption for measuring fluctuations from it), then we can show by taking expectations in \eqref{weak_noise_OU_solution} and using results presented in \ref{intro_SDE} that we must have $\mathbb{E}[Y_t | Y_0] = 0$. In other words, the fluctuations have zero expectation and are expected to occur symmetrically about $\alpha(t)$), with no bias. The variance (spread) of the fluctuations $Y_t$, as well as higher moments, can also be exactly calculated from \eqref{weak_noise_OU_solution} using some tools from stochastic calculus, but we will not demonstrate this here.\\
\\
Importantly, higher order terms do not form FPEs, and in general, $\Tilde{P}_r$ for $r>0$ may be negative and therefore does not even describe a probability. As such, formulating the solution as the solution to an SDE only works for $\Tilde{P}_0$. If noise is large enough that it is not well-approximated by $\Tilde{P}_0$, this method is not very useful.

% \subsection{An example: The stochastic logistic equation}
% Consider the functional forms of example \ref{ex_1D_stoch_logistic}, given by
% \begin{equation}
% \label{ex_1D_stoch_logistic_BD_eqns}
% \begin{aligned}
%     b(n) &= \lambda n\\
%     d(n) &= \left(\mu + (\lambda-\mu)\frac{n}{K}\right)n
% \end{aligned}
% \end{equation}
% Here, $K$ is the system-size parameter. Introducing the new variable $x=n/K$, we obtain
% \begin{align*}
%     b_K(x) &= \frac{1}{K}b(n) = \frac{1}{K}\lambda Kx\\
%     d_K(x) &= \frac{1}{K}d(n) = \frac{1}{K}\left(\mu + (\lambda-\mu)\frac{Kx}{K}\right)Kx
% \end{align*}
% Thus, we have
% \begin{equation*}
%     A^{\pm}(x) = b_K(x)\pm d_K(x) = x\left(\lambda \pm \left(\left(\mu + (\lambda-\mu)x\right)\right) \right)
% \end{equation*}
% Defining $r=\lambda-\mu$ and $v=\lambda+\mu$, we see that the deterministic dynamics are
% \begin{equation}\label{ex_1D_stoch_logistic_det_limit}
%     \frac{dx}{dt} = A^-(x) = rx(1-x)
% \end{equation}
% showing that in the infinite population limit, we obtain the logistic equation. The noise is controlled by
% \begin{equation*}
%     \sqrt{\frac{A^+(x)}{K}} = \sqrt{\frac{x(v+rx)}{K}}
% \end{equation*}
% and thus, the diffusion approximation (`mesoscopic view') of the system is given by the solution of the SDE
% \begin{equation}\label{ex_1D_stoch_logistic_full_SDE}
% dX_t =  rX_t(1-X_t)dt + \sqrt{\frac{X_t(v+rX_t)}{K}}dB_t
% \end{equation}
% Letting $\alpha(t)$ be the solution of the logistic equation \eqref{ex_1D_stoch_logistic_det_limit}, We can taylor expand $A^{\pm}(x)$ for the weak noise approximation, and we find:
% \begin{align*}
%     A^-_1(x) &= \frac{d}{dx}(rx(1-x))\biggl{|}_{x=\alpha} = r(1 - 2\alpha(t))\\
%     A^+_0(x) &= \alpha(t)(v+r\alpha(t))
% \end{align*}
% Thus, the weak noise approximation of \ref{ex_1D_stoch_logistic_BD_eqns} is given by
% \begin{equation}\label{ex_1D_stoch_logistic_WNA}
%     X_t = \alpha(t) + \frac{1}{\sqrt{K}}Y_t
% \end{equation}
% where the stochastic process $Y_t$ is an Ornstein-Uhlenbeck process given by the solution to the linear SDE
% \begin{align}
%     dY_t &= A^-_1(t)Y_tdt + \sqrt{A^+_0(t)}dB_t\nonumber\\
%     \Rightarrow dY_t &= r(1 - 2\alpha(t))Y_tdt + \sqrt{\alpha(t)(v+r\alpha(t))}dB_t\label{ex_1D_stoch_logistic_WNA}
% \end{align}
% The deterministic trajectory \eqref{ex_1D_stoch_logistic_det_limit} has two fixed points, one at $x=0$ (extinction) and one at $x=1$ (corresponding to a population size of $n=K$). For $r > 0$, $x=0$ is unstable and $x=1$ is a global attractor, meaning in the deterministic limit, when $r > 0$, all populations end up at $x=1$ given enough time. The stochastic dynamics \eqref{ex_1D_stoch_logistic_full_SDE} and \eqref{ex_1D_stoch_logistic_WNA} depend not only on $r$, but also on $v$, the sum of the birth and death rates. It has been proven that $X_t = 0$ is the only recurrent state for the full stochastic dynamics \eqref{ex_1D_stoch_logistic_full_SDE}, meaning that every population is guaranteed to go extinct\footnote{This can be proven using tools from Markov chain theory. For those interested, the proof uses ergodicity to arrive at a contradiction if any state other than $0$ exhibits a non-zero density at steady state.} given enough time \citep{nasell_extinction_2001}, thus illustrating an important difference between finite and infinite populations. $X_t = 0$ is also an `absorbing' state since once a population goes extinct, it has no way of being revived in this model. However, if $K$ is large enough, the eventual extinction of the population may take a very long time. In fact, we can make the expected time to extinction arbitrarily long by making $K$ sufficiently large. Thus, for moderately large values of $K$, it is biologically meaningful only to look at a weaker version of the steady state distribution by imposing the condition that the population does not go extinct and looking at the `transient' dynamics \citep{hastings_transients_2004}. Conditioned on non-extinction, the solution to \eqref{ex_1D_stoch_logistic_full_SDE} has a `quasistationary' distribution about the deterministic attractor $X_t = 1$, with some variance reflecting the effect of noise-induced fluctuations in population size \citep{nasell_extinction_2001} due to the finite size of the population. The weak-noise approximation \eqref{ex_1D_stoch_logistic_WNA} implicitly assumes non-extinction by only measuring small fluctuations from the deterministic solution to \eqref{ex_1D_stoch_logistic_det_limit} and thus, at steady state, naturally describes a quasistationary distribution centered about $X_t = 1$.

\section{Multi-dimensional processes for discrete traits}\label{sec_nD_processes}
Let us now consider a slightly more complicated scenario. Assume that our population is \emph{not} composed of identical organisms, but instead can contain up to $m$ different kinds of organisms - For example, individuals may come in one of $m$ colors, or a gene may have $m$ different alleles. The formalism we have developed in the previous section carries out essentially unchanged in this case.

\subsection{Description of the process and the Master Equation}
Given a population that can contain up to $m$ different (fixed) kinds of organisms, it can be entirely characterized by specifying the number of organisms of each type (Figure \ref{fig_nD_pop_description}A). Thus, the state of the population at a given time $t$ is an $m$-dimensional \emph{vector} of the form $\mathbf{v} = [v_1(t),v_2(t),\ldots,v_m(t)]$, where $v_i(t)$ is the number of individuals of type $i$.\\
Given a state $\mathbf{v}(t)$,  we also need to describe how this vector can change over time due to births and deaths (ecology). In this case, a birth or death could result in an individual belonging to one of $m$ different types. Thus, whereas before we had two functions $b(n)$ and $d(n)$ which take in a number as an input, we now require $2m$ functions that take in a vector as an input (Figure \ref{fig_nD_pop_description}B). In other words, for each type $i \in \{1,2,\ldots,m\}$, we must specify a birth rate $b_i(\mathbf{v})$ and a death rate $d_i(\mathbf{v})$. By `rates', we mean that if we know that \emph{either a birth or a death} occurs, then the probability that this event is the birth of an individual of type $i$ is given by
\begin{equation*}
\mathbb{P}[\textrm{ Birth of a type $i$ individual} | \textrm{ something happened }] = \frac{b_i(\mathbf{v})}{\sum\limits_{j=1}^{m}(b_j(\mathbf{v})+d_j(\mathbf{v}))}
\end{equation*}
and the probability that the event is the death of an individual of type $i$ is
\begin{equation*}
\mathbb{P}[\textrm{ Death of a type $i$ individual} | \textrm{ something happened }] = \frac{d_i(\mathbf{v})}{\sum\limits_{j=1}^{m}(b_j(\mathbf{v})+d_j(\mathbf{v}))}
\end{equation*}

\myfig{0.9}{Media/3.2_BD_process_2D.png}{\textbf{(A)} Consider a population of birds in which individuals are either red or blue. In this case, we have $m=2$, since there are two types of individuals in the population. \textbf{(B)} The state of the system can be described by a vector containing the number of individuals of each discrete type (in this case, the number of red and blue birds in the population). Births and deaths result in changes in the elements of the state vector.}{fig_nD_pop_description}

As before, we can describe the rate of change of $P(\mathbf{v},t)$, the probability of finding the population in a state $\mathbf{v}$ at time $t$, by measuring the inflow and outflow rates. Given a population $\mathbf{v} = [v_1,\ldots,v_{m}]$, the `inflow' is from all populations of the form $[v_1,\ldots,v_{i}-1,\dots,v_{m}]$ through a birth of a type $i$ individual, and from all populations of the form $[v_1,\ldots,v_{i}+1,\dots,v_{m}]$ through the death of a type $i$ individual. Thus, we have the inflow rate
\begin{equation}
\label{nD_rate_in}
\begin{split}
R_{\textrm{in}}(\mathbf{v},t) &= \sum\limits_{j=1}^{m}b_{j}([v_1,\ldots,v_{j}-1,\ldots,v_m])P([v_1,\ldots,v_{j}-1,\ldots,v_m],t) \\
& +\sum\limits_{j=1}^{m}d_{j}([v_1,\ldots,v_{j}+1,\ldots,v_m])P([v_1,\ldots,v_{j}+1,\ldots,v_m],t)
\end{split}
\end{equation}
Outflow is through births and deaths of individuals in the population $\mathbf{v}$ itself, and thus we have:
\begin{equation}
\label{nD_rate_out}
R_{\textrm{out}}(\mathbf{v},t) = \sum\limits_{j=1}^{m}b_{j}(\mathbf{v})P(\mathbf{v},t) + \sum\limits_{j=1}^{m}d_{j}(\mathbf{v})P(\mathbf{v},t)
\end{equation}
As before, we now define step operators, both for notational ease and in anticipation of the system size expansion. Note that now, we need $2m$ step operators. For each $i \in \{1,\ldots,m\}$, let us define two step operators $\mathcal{E}_{i}^{\pm}$ by their action on any function $f([v_1,\ldots,v_m],t)$ as:
\begin{equation}
\label{nD_step_operators}
\mathcal{E}_{i}^{\pm}f([v_1,\ldots,v_m],t) = f([v_1,\ldots,v_i \pm 1, \ldots v_m],t)
\end{equation}
In other words, $\mathcal{E}_{i}^{\pm}$ just changes the population through the addition or removal of one type $i$ individual. We can now the rate of change of $P(\mathbf{v},t)$ as
\begin{equation}
\frac{\partial P}{\partial t}(\mathbf{v},t) = R_{\textrm{in}}(\mathbf{v},t) - R_{\textrm{out}}(\mathbf{v},t)
\end{equation}
Substituting \eqref{nD_rate_in}, \eqref{nD_rate_out}, and \eqref{nD_step_operators}, we obtain:
\begin{equation}
\label{nD_M_eqn}
\frac{\partial P}{\partial t}(\mathbf{v},t) = \sum\limits_{j=1}^{m}\left[(\mathcal{E}_j^{-}-1)b_j(\mathbf{v})P(\mathbf{v},t) + (\mathcal{E}_j^{+}-1)d_j(\mathbf{v})P(\mathbf{v},t)\right]
\end{equation}
This is the master equation of our $m$-dimensional process.

\subsection{The system-size expansion}
As before, we now assume we can find a system size parameter $K > 0$ such that we can make the substitutions
\begin{align*}
\mathbf{x} &= \frac{\mathbf{v}}{K}\\
b^{(K)}_i(\mathbf{x}) &= \frac{1}{K}b_i(\mathbf{v})\\
d^{(K)}_i(\mathbf{x}) &= \frac{1}{K}d_i(\mathbf{v})
\end{align*}
and define new step operators $\Delta_{i}^{\pm}$ by their action on any real-valued function $f(\mathbf{x},t)$ as
\begin{equation}
\label{nD_step_operators_rescaled}
\Delta_{i}^{\pm}f([x_1,\ldots,x_m],t) = f([x_1,\ldots,x_i \pm \frac{1}{K}, \ldots x_m],t)
\end{equation}
In terms of these new variables, \eqref{nD_M_eqn} becomes
\begin{equation}
\label{nd_M_eqn_rescaled}
\frac{\partial P}{\partial t}(\mathbf{x},t) = K\sum\limits_{j=1}^{m}\left[(\Delta_j^{-}-1)b^{(K)}_j(\mathbf{x})P(\mathbf{x},t) + (\Delta_j^{+}-1)d^{(K)}_j(\mathbf{x})P(\mathbf{x},t)\right]
\end{equation}
If $K$ is large, we can once again Taylor expand the action of the step operators as
\begin{equation*}
f([x_1,\ldots,x_i \pm \frac{1}{K}, \ldots x_m],t) = f(\mathbf{x},t) \pm \frac{1}{K}\frac{\partial f}{\partial x_i}(\mathbf{x},t) + \frac{1}{2K^2}\frac{\partial^2f}{\partial x_i^2}(\mathbf{x},t) + \mathcal{O}(K^{-3})
\end{equation*}
which, after substituting into \eqref{nd_M_eqn_rescaled}, yields the equation
\begin{equation}
\label{nD_FPE}
\setlength{\fboxsep}{2\fboxsep}\boxed{\frac{\partial P}{\partial t}(\mathbf{x},t) = \sum\limits_{j=1}^{m}\left[-\frac{\partial}{\partial x_j}\{A_j^{-}(\mathbf{x})P(\mathbf{x},t)\} + \frac{1}{2K}\frac{\partial^2}{\partial x_j^2}\{A_j^{+}(\mathbf{x})P(\mathbf{x},t)\}\right]}
\end{equation}
where
\begin{equation*}
A_{i}^{\pm}(\mathbf{x}) = b^{(K)}_i(\mathbf{x})\pm d^{(K)}_i(\mathbf{x})
\end{equation*}
Equation \eqref{nD_FPE} is an $m$-dimensional Fokker-Planck equation, and corresponds to the $m$-dimensional It\^o process
\begin{equation}
\label{nD_Ito_SDE}
d\mathbf{X}_{t} = \mathbf{A^-}(\mathbf{X}_t)dt + \frac{1}{\sqrt{K}}\mathbf{D}(\mathbf{X}_t)d\mathbf{B}_t
\end{equation}
where $\mathbf{A^-}(\mathbf{X}_t)$ is the $m$ dimensional `drift vector' with $i$\textsuperscript{th} element $ = A^{-}_{i}(\mathbf{X}_t)$. $\mathbf{D}(\mathbf{X}_t)$ is the $m \times m$ `diffusion matrix' with $ij$th element $\left(\mathbf{D}(\mathbf{X}_t)\right)_{ij} = \delta_{ij}\left(A^{+}_{i}A^{+}_{j}\right)^{\frac{1}{4}}$, where $\delta_{ij}$ is the Kronecker delta symbol, defined by
\begin{equation*}
\delta_{ij} = 
\begin{cases}
1 & i=j\\
0 & i\neq j
\end{cases}
\end{equation*}
Finally, $\mathbf{B}_t$ is the $m$-dimensional Brownian motion and can be thought of as a vector of independent one-dimensional Brownian motions (which have been defined in \ref{intro_SDE}). This is the `mesoscopic' description of our process.

\subsection{The deterministic limit}
Once again, we can take $K \to \infty$ in \eqref{nD_Ito_SDE} to obtain a deterministic expression. Here, the expression reads
\begin{equation}
\label{nD_det_limit}
\frac{d\mathbf{x}}{dt} = \mathbf{A^-}(\mathbf{x}) = \mathbf{b}^{(K)}(\mathbf{x}) - \mathbf{d}^{(K)}(\mathbf{x})
\end{equation}
where the $m$-dimensional vector-valued functions $\mathbf{b}^{(K)}(\mathbf{x})$ and $\mathbf{d}^{(K)}(\mathbf{x})$ on the RHS are defined as having $i$\textsuperscript{th} element $b^{(K)}_i(\mathbf{x})$ and $d^{(K)}_i(\mathbf{x})$ respectively. This deterministic limit is sometimes called the `macroscopic' description.
\subsubsection{Some familiar faces: Replicator-mutator, quasispecies equation, and the Price equation}
In this multi-dimensional case, the macroscopic description amounts to a description of evolutionary game theory under mild assumptions on the birth and death rate vectors. We show this below, using methods first outlined by \citep{page_unifying_2002}. Let us assume that we can write the birth and death rate vectors as
\begin{equation}
\label{nD_functional_forms_for_replicator}
\begin{aligned}
b^{(K)}_i(\mathbf{x}) &= \mu Q_i(\mathbf{x}) + b^{(\textrm{ind})}_{i}(\mathbf{x})x_i\\
d^{(K)}_i(\mathbf{x}) &= d^{(\textrm{ind})}_i(\mathbf{x})x_i
\end{aligned}
\end{equation}
where $\mu \geq 0$ is a constant describing the mutation rate, $Q_i(\mathbf{x})$ is a real-valued function that describes the birth rate of type $i$ individuals due to mutations in the population $\mathbf{x}$, and $b^{(\textrm{ind})}_{i}(\mathbf{x})$ and $d^{(\textrm{ind})}_{i}(\mathbf{x})$ are real-valued functions that respectively describe the per-capita birth and death rate of type $i$ individuals. Our assumptions of the functional forms \eqref{nD_functional_forms_for_replicator} thus amount to saying that birth and death rates can be separated into mutational and non-mutational components, and furthermore that the density dependence of the birth and death rates due to non-mutational effects is in a form that allows us to write down per-capita birth and death rates. Plugging these definitions into \eqref{nD_det_limit} and writing it down component-wise, we obtain the equation
\begin{equation}
\label{nD_det_limit_fitess_defn}
\frac{dx_i}{dt} = w_i(\mathbf{x})x_i + \mu Q_i(\mathbf{x})
\end{equation}
Where for each $i$, we have defined the real-valued function $w_i(\mathbf{x}) \coloneqq b^{(\textrm{int})}_{i}(\mathbf{x}) - d^{(\textrm{int})}_i(\mathbf{x})$. Setting the mutation rate $\mu \to 0$ in equation \eqref{nD_det_limit_fitess_defn} makes it clear that the quantity $w_i(\mathbf{x})x_i$ describes the growth rate of type $i$ individuals in the population due to all processes other than mutation. The quantity $w_i(\mathbf{x})$ thus describes the per-capita growth rate of type $i$ individuals in a population $\mathbf{x}$, and is sometimes called the `Malthusian fitness' of type $i$. Ecologists often denote this quantity by the symbol $r_i$ and simply call it the (exponential) growth rate of type $i$, but we will stick to $w_i$ and `fitness' here. It is notable that the fitness of a type depends on the state of the population as a whole (\textit{i.e.} $\mathbf{x}$) and is thus frequency-dependent.\\
\\
Given a state $\mathbf{x}(t)$, we can now compute the total (scaled) population size and the frequency of each type in the population as:
\begin{equation}
\label{nD_tot_pop_and_prop_inds_defn}
\begin{aligned}
N_{K}(t) &\coloneqq \sum\limits_{i=1}^{m}x_i(t)\\
p_i(t) &\coloneqq \frac{x_i(t)}{N_{K}(t)}
\end{aligned}
\end{equation}
We can also calculate the statistical mean value of any type level quantity $f$ in the population as
\begin{equation}
\label{nD_mean}
\overline{f}(t) \coloneqq \sum\limits_{i=1}^{m}f_ip_{i}(\mathbf{x}(t))
\end{equation}
, where $f_i$ is the value of the quantity for the $i$th type.\\
\\
We can now compute the rate of change of $p_i$, the proportion of type $i$ individuals in the population:
\begin{align}
\frac{dp_i}{dt} &= \frac{1}{N_{K}(t)}\frac{dx_i}{dt} - \frac{x_i}{N_{K}^2(t)}\frac{dN_{K}}{dt}\nonumber\\
&= \frac{1}{N_{K}(t)}\frac{dx_i}{dt} - \frac{x_i}{N_{K}^2(t)}\sum\limits_{j=1}^{m}\frac{dx_j}{dt}\label{nD_replicator_intermediate_1}
\end{align}
Substituting \eqref{nD_det_limit_fitess_defn} into \eqref{nD_replicator_intermediate_1}, we now obtain
\begin{align*}
\frac{dp_i}{dt} &=  \frac{1}{N_{K}(t)}\left[w_i(\mathbf{x})x_i + \mu Q_i(\mathbf{x})\right] - \frac{x_i}{N_{K}^2(t)}\sum\limits_{j=1}^{m}\left[w_j(\mathbf{x})x_j + \mu Q_j(\mathbf{x})\right]\\
&= w_i(\mathbf{x})p_i + \frac{\mu}{N_K}Q_i(\mathbf{x}) - p_i\sum\limits_{j=1}^{m}\left[w_j(\mathbf{x})p_j + \frac{\mu}{N_K}Q_j(\mathbf{x})\right]
\end{align*}
Where we have used the definition of $p_i$ from \eqref{nD_tot_pop_and_prop_inds_defn}. Now using the definition of mean fitness from \eqref{nD_mean} and rearranging terms, we obtain
\begin{equation}
\label{nD_replicator_mutator}
\setlength{\fboxsep}{2\fboxsep}\boxed{\frac{dp_i}{dt} = (w_i(\mathbf{x}) - \overline{w})p_i + \mu\left[Q_i(\mathbf{p}) - p_i\left(\sum\limits_{j=1}^{m}Q_j(\mathbf{p})\right)\right]}
\end{equation}
Where we have used the notation $Q_i(\mathbf{p}) = Q_i(\mathbf{x})/N_K(t)$ for notational clarity. The first term of \eqref{nD_replicator_mutator} describes changes due to faithful (non-mutational) replication, and the second describes changes due to mutation. For this reason, equation \eqref{nD_replicator_mutator} is called the \emph{replicator-mutator equation} in the evolutionary game theory literature, where the individual `types' are interpreted to be pure strategies. If in addition, each $w_i(\mathbf{x})$ is linear in $\mathbf{x}$, meaning we can write $w_i(\mathbf{x}) = \sum_{j}a_{ij}x_j$ for some set of constants $a_{ij}$, then we get the replicator-mutator equation for matrix games, and the constants $a_{ij}$ form the `payoff matrix'. As is well-known, the replicator equation (without mutation) for matrix games with $m$ pure strategies is equivalent to the generalized Lotka-Volterra equations for a community with $m-1$ species\citep{hofbauer_evolutionary_1998}, providing the connection to community ecology.  Equation \eqref{nD_replicator_mutator} is also equivalent to Eigen's \emph{quasispecies equation} from molecular evolution if each `type' is interpreted as a genetic sequence and each $w_i(\mathbf{x})$ is a constant function\footnote{Mutational effects are often additionally assumed to act through direct `transmission probabilities' of mutating from one type to another. This means that we can write $Q_i(\mathbf{p}) = \sum\limits_{j\neq i}Q_{ij}p_j$, where $Q_{ij} \geq0$ is a constant describing the probability of a $j \to i$ mutation (conditioned on the occurrence of a mutation). Substituting this into \eqref{nD_replicator_mutator} yields an equation in terms of `$Q$-matrices' or `mutation matrices' that may be more familiar to some biologists.}. We can now calculate how the mean of any `type level' quantity $f$, defined as $f_i$ for the $i$\textsuperscript{th} type, changes in the population (For example, if each type is a phenotype for a trait such as height, which can be assigned a numerical value, then setting $f_i = \textit{value of $i\textsuperscript{th}$ phenotype}$ gives us the mean trait value in the population). Multiplying both sides of equation \eqref{nD_replicator_mutator} by $f_i$ and summing over all $i$, we obtain
\begin{align*}
\frac{d}{dt}\left(\sum\limits_{i=1}^{m}f_ip_i\right) &= \sum\limits_{i=1}^{m}f_iw_i(\mathbf{x})p_i - \overline{w}\sum\limits_{i=1}^{m}f_ip_i + \mu\left[\sum\limits_{i=1}^{m}Q_i(\mathbf{p})f_i - \left(\sum\limits_{j=1}^{m}Q_j(\mathbf{p})\sum\limits_{i=1}^{m}p_if_i\right)\right]\\
\Rightarrow \frac{d\overline{f}}{dt} &= \overline{wf}-(\overline{w})(\overline{f}) + \mu\left[\sum\limits_{i=1}^{m}Q_i(\mathbf{p})f_i - \left(\sum\limits_{j=1}^{m}Q_j(\mathbf{p})\right)\overline{f}\right]
\end{align*}
Using the definition of statistical covariance of two variables $X$ and $Y$ as $\mathrm{Cov}(X,Y) = \overline{XY} - (\overline{X})(\overline{Y})$, we obtain
\begin{equation}
\label{nD_Price}
\frac{d\overline{f}}{dt} = \mathrm{Cov}(w,f) + \mu\left[\sum\limits_{i=1}^{m}Q_i(\mathbf{p})f_i - \left(\sum\limits_{j=1}^{m}Q_j(\mathbf{p})\right)\overline{f}\right]
\end{equation}
The first term of the RHS describes the statistical covariance between the quantity $f$ and the fitness $w$. The second term describes `transmission bias' due to mutational effects - The first summation is the `inflow' of $f$ due to mutations, and the second is the `outflow'. Equation \eqref{nD_Price} is thus a version of the Price equation.

\subsection{Stochastic trait frequency dynamics for finite populations}

Following ideas similar in spirit to \citep{mcleod_social_2019}, we can also use \eqref{nD_Ito_SDE} to write down stochastic equations for $p_i(t)$, the frequency of type $i$ individuals in the population to contrast with the replicator-mutator equation \eqref{nD_replicator_mutator} derived above. In appendix \ref{App_density_to_freq}, we show that we can use It\^{o}'s formula to write  down a general stochastic equation for the frequencies of each type in the population. Unlike \citep{mcleod_social_2019}, we make no assumptions about the separation of ecological and evolutionary time scales or the strength of selection and are able to present an entirely general calculation. Letting $\tau_i(\mathbf{x}) = b^{(\textrm{int})}_{i}(\mathbf{x}) + d^{(\textrm{int})}_i(\mathbf{x})$ be the \emph{per-capita turnover rate} of the $i\textsuperscript{th}$ type and denoting $\overline{\tau} = \sum_i p_i\tau_i$, we show in appendix \ref{App_density_to_freq} that when the birth and death rates are defined by \eqref{nD_functional_forms_for_replicator}, then the frequency of the $i\textsuperscript{th}$ type in the population $\mathbf{X}_t$, changes according to the equation:
\begin{equation}
\label{nD_eqn_for_frequencies}
\begin{aligned}
dp_i(t) &= \underbrace{\left[(w_i(\mathbf{x}) - \overline{w})p_i + \mu\left\{Q_i(\mathbf{p}) - p_i\left(\sum\limits_{j=1}^{m}Q_j(\mathbf{p})\right)\right\}\right]dt}_{\substack{\text{Infinite population predictions} \\ \text{(Replicator-mutator, Price, etc.)}}}\\
&- \frac{1}{K}\underbrace{\frac{1}{N_{K}(t)}\left[(\tau_i(\mathbf{x}) - \overline{\tau})p_i + \mu\left\{Q_i(\mathbf{p}) - p_i\left(\sum\limits_{j=1}^{m}Q_j(\mathbf{p})\right)\right\}\right]dt}_{\substack{\text{Directional finite population effects}\\\text{due to differential turnover rates}}}\\
&+ \frac{1}{\sqrt{K}}\underbrace{\left[\left(A^{+}_{i}\right)^{1/2}dB^{(i)}_t - p_i\sum\limits_{j=1}^{m}\left(A^{+}_{j}\right)^{1/2}dB^{(j)}_t\right]}_{\substack{\text{Non-directional finite population effects}\\\text{due to stochastic fluctuations}}}
\end{aligned}
\end{equation}
where $B^{(1)}_t,B^{(2)}_t, \ldots, B^{(m)}_t$ are $m$ independent one-dimensional standard Brownian motion processes. The first term in this expression is the same as equation \eqref{nD_replicator_mutator} and describes directional changes in the population composition due to `classical' evolutionary forces such as selection and mutation. We saw in the previous section that the replicator-mutator equation is generically satisfied in the infinite population limit for all birth-death processes that are in the (very general) functional forms \eqref{nD_functional_forms_for_replicator}, a result that is made explicit in \eqref{nD_eqn_for_frequencies} (just take $K \to \infty$ to convince yourself). The second term is an additional directional force on population composition that is only seen in finite populations. Mathematically, this term acts exactly analogously to the replicator equation but in the opposite direction (A higher relative $\tau_i$ leads to a decrease in frequency - notice the minus sign before the second term in \eqref{nD_eqn_for_frequencies}). Biologically, this term describes a biasing effect due to differential turnover rates and can intuitively be understood through the following reasoning: If a type $i$ has a higher $\tau_i$, it experiences greater turnover due to a generally higher birth and death rate and thus experience more births and deaths in a given time interval than an otherwise equivalent species with a lower $\tau_i$. More events mean greater demographic stochasticity, and types with a higher $\tau_i$ thus tend to be eliminated by a stochastic analog of selection because they experience more chance events (births and deaths) in a given time period. This effect is less visible if the total population size is higher because larger populations generally experience less stochasticity, which is reflected in the $1/N_K$ factor in this term. This stochastic analog of selection for reduced turnover rates is the force responsible for the `reversal of the direction of deterministic selection' induced by demographic noise in previous studies \citep{constable_demographic_2016, mcleod_social_2019}. Note that types that tend to increase the \emph{total} population size $N_K(t)$ (such as altruists in evolutionary theory and mutualists in ecological communities) will reduce the magnitude of this effect compared to types that do not facilitate such an increase (such as cheaters and highly competitive species), which could explain why the former routinely persist for long periods in finite population models despite extinction being predicted in infinite population models. The fact that the entire term is multiplied by $(KN_K(t))^{-1}$ suggests that the effect of this force is weak for medium to large populations, which explains why the persistence of cooperators is often only observed in restrictive sounding conditions such as quasi-neutrality, rapid attraction to a slow manifold, or a weak selection + weak mutation limit. In all three of these cases, the first term on the RHS of \eqref{nD_eqn_for_frequencies} becomes identically 0. It therefore no longer contributes to the total dynamics and allows us to see the contributions of the second term.\\
Finally, the last term of equation \eqref{nD_eqn_for_frequencies} describes non-directional stochastic effects due to fluctuations and has a `spreading effect'. To the highest order in $\sqrt{K}$, the effect of these fluctuations can be described through the weak noise approximation, as we explain below. However, the $1/K$ factor in the second term of \eqref{nD_eqn_for_frequencies} means that the effect of differential turnover rates disappears in this weak noise regime  (and also in other such approximate treatments, such as the Chemical Langevin equation).

\subsection{Stochastic fluctuations and the weak noise approximation}

If the noise is \emph{weak}, we can go a little further, as in the one-dimensional case. Let the deterministic trajectory obtained by solving \eqref{nD_det_limit} be given by $\mathbf{a}(t)$.  We can once again track 
stochastic fluctuations from the deterministic trajectory by introducing the new variables
\begin{equation}
\begin{aligned}
\mathbf{y} &= \sqrt{K}(\mathbf{x} - \mathbf{a}(t))\\
s&=t\\
\tilde{P}(\mathbf{y},s) &= \frac{1}{\sqrt{K}}P(\mathbf{x},t)
\end{aligned}
\end{equation}
Then, after some algebra that follows the exact same steps as in section \ref{sec_1D_WNA} and retaining only the highest order terms in $\sqrt{K}$, we obtain the equation:
\begin{equation}
\label{nD_WNA_intermediate}
\frac{\partial \Tilde{P}_{0}}{\partial s}(\mathbf{y},s) = \sum\limits_{j=1}^{m}\left(-\frac{\partial}{\partial y_j}\left\{(A^{-}_{j})_{1}(s)\Tilde{P}_{0}(\mathbf{y},s)\right\}+\frac{1}{2}{A_j}^{+}(\mathbf{a}(s))\frac{\partial^2}{\partial{y_j}^2}\{\Tilde{P}_{0}(\mathbf{y},s)\}\right)
\end{equation}
where $(A^{-}_{j})_{1}(s)$ is the $\mathcal{O}(1/\sqrt{K})$ term of the power series expansion
\begin{equation*}
A^-_{j}(\mathbf{a} + \frac{\mathbf{y}}{\sqrt{K}}) = \sum\limits_{n=1}^{\infty}(A^{-}_{j})_{n}(s)\left(\frac{\mathbf{y}}{\sqrt{K}}\right)^n
\end{equation*}
In the case where the series expansion is a Taylor expansion, then the first-order term of this expansion is given by
\begin{equation}
\label{nD_WNA_taylor_term}
(A^{-}_{j})_{1}(s) = \sum\limits_{i=1}^{m} y_i\left(\frac{\partial A^{-}_j(\mathbf{x})}{\partial x_i}\bigg{|}_{\mathbf{x}=\mathbf{a}(s)}\right)
\end{equation}
In multi-variable calculus, the directional derivative\footnote{Physicists sometimes use the notation $\partial_{\mathbf{v}}f(\mathbf{x})$ or $\mathbf{v}\cdot\nabla f(\mathbf{x})$ for this object.} $D_{\mathbf{v}}(f(\mathbf{x}))$ of a multidimensional function $f: \mathbb{R}^n \to \mathbb{R}$ along a vector $\mathbf{v}$ is the function defined by:
\begin{equation}
\label{directional_derivative_defn}
D_{\mathbf{v}}(f(\mathbf{x})) \coloneqq \sum\limits_{i=1}^{n}\left(\frac{\partial f(\mathbf{x})}{\partial x_i}\right)v_i = \lim_{h \to 0}\frac{f(\mathbf{x}+h\mathbf{v})-f(\mathbf{x})}{h}
\end{equation}
Comparing with \eqref{nD_WNA_taylor_term}, we see that the weak-noise approximation of our process is:
\begin{equation}
\label{nD_WNA}
\frac{\partial P}{\partial t}(\mathbf{y},t) = \sum\limits_{j=1}^{m}\left(-\frac{\partial}{\partial y_j}\left\{D_{\mathbf{y}}(A_j^-(\mathbf{a}))(t)P(\mathbf{y},t)\right\}+\frac{1}{2}{A_j}^{+}(\mathbf{a}(t))\frac{\partial^2}{\partial{y_j}^2}\{P(\mathbf{y},t)\}\right)
\end{equation}
where we have dropped the tildes and gone back from $s$ to $t$ for notational clarity. The directional derivative of the population turnover rate $A_j^-$ `in the direction' of the stochastic fluctuation $\mathbf{y}$ at the deterministic point $\mathbf{a}(s)$ here is the multidimensional analogue of the derivative we had in \eqref{1D_WNA}. The meaning of equation \eqref{nD_WNA} is clearer if we compute how the moments of the fluctuation $y_i$ in the density of type $i$ individuals (for some $i$) change over time. Let $n > 0$. We have:
\begin{align}
\frac{d}{dt}\mathbb{E}[y_i^n] &= \frac{d}{dt}\int\limits_{\mathbb{R}^m}y_i^nP(\mathbf{y},t)d\mathbf{y}\\
&= \int\limits_{\mathbb{R}^m}y_i^n\frac{\partial P}{\partial t}(\mathbf{y},t)d\mathbf{y}\label{nD_change_of_moments_defn}
\end{align}
where we have assumed that $y_i^n$ and $P(\mathbf{y},t)$ vary sufficiently smoothly to allow us to interchange the order of derivatives and integrals and used the shorthand $\displaystyle \int\limits_{\mathbb{R}^m} \ f(\mathbf{y}) \ d\mathbf{y} = \int\limits_{\mathbb{R}}\int\limits_{\mathbb{R}}\cdots\int\limits_{\mathbb{R}} \ f(\mathbf{y}) \ dy_1 dy_2 \ldots dy_m$. The one-dimensional integrals are over the entire real line and not just over $[0,\infty)$ because fluctuations can be either positive (greater than $\mathbf{a}(t)$) or negative (lesser than $\mathbf{a}(t)$). For notational brevity, let us use the shorthand $D_j = D_{\mathbf{y}}(A_j^-(\mathbf{a}))(t)$. We can now substitute \eqref{nD_WNA} into \eqref{nD_change_of_moments_defn} to obtain
\begin{align}
\frac{d}{dt}\mathbb{E}[y_i^n] &= \int\limits_{\mathbb{R}^m} y_i^n \left(\sum\limits_{j=1}^{m}\left(-\frac{\partial}{\partial y_j}\left\{D_{j}P(\mathbf{y},t)\right\}+\frac{1}{2}{A_j}^{+}(\mathbf{a}(t))\frac{\partial^2}{\partial{y_j}^2}\{P(\mathbf{y},t)\}\right)\right)d\mathbf{y}\\
&= \sum\limits_{j=1}^{m}\left[-\int\limits_{\mathbb{R}^m} y_i^n\frac{\partial}{\partial y_j}\left\{D_{j}P(\mathbf{y},t)\right\}d\mathbf{y} + \frac{{A_j}^{+}(\mathbf{a}(t))}{2}\int\limits_{\mathbb{R}^m} y_i^n\frac{\partial^2}{\partial{y_j}^2}\{P(\mathbf{y},t)\}d\mathbf{y}\right]\label{nD_intermediate_for_moments}
\end{align}
We will evaluate the integrals on the RHS of \eqref{nD_intermediate_for_moments} using integration by parts. Recall that for any two functions $u$ and $v$ defined on a domain $\Omega$, the general formula for integration by parts is given by:
\begin{equation}
\label{int_by_parts_general_formula}
\int\limits_{\Omega}\frac{\partial u}{\partial x_i}vd\mathbf{x} = -\int\limits_{\Omega}u\frac{\partial v}{\partial x_i}d\mathbf{x} + \int\limits_{\partial\Omega}uv\gamma_{i}dS(\mathbf{x})
\end{equation}
where $\partial \Omega$ is the boundary of $\Omega$, $dS$ is the surface element of this boundary, and $\gamma_i$ is the $i\textsuperscript{th}$ component of the unit outward normal to the boundary. In our case, we have $\Omega = \mathbb{R}^m$, and thus the boundary conditions are evaluated as $\|y\| \to \infty$. We assume that the magnitude of stochastic fluctuations is bounded, and therefore impose the condition $\displaystyle \lim_{\|y\| \to \infty}  P(\mathbf{y},t) = 0$. Further, we assume that this decay is fast enough that $\displaystyle \lim_{\|y\| \to \infty}D_jP(\mathbf{y},t) = 0\ \forall \ j$. Under these conditions, we can evaluate the two integrals in the RHS of \eqref{nD_intermediate_for_moments} by using integration by parts and discarding the boundary term (The second term on the RHS of \eqref{int_by_parts_general_formula}). Note that since the $y_i$s are orthogonal to each other, we have the relation:
\begin{equation*}
\frac{\partial y_i ^{n}}{\partial y_j} = \delta_{ij}ny_i^{n-1}
\end{equation*}
Using this relation and then using integration by parts on the RHS of \eqref{nD_intermediate_for_moments} (once for the first term and twice for the second term), we obtain the considerably simpler expression
\begin{align}
\frac{d}{dt}\mathbb{E}[y_i^n] &= n\int\limits_{\mathbb{R}^m} y_i^{n-1}D_{i}P(\mathbf{y},t)d\mathbf{y} + \frac{n(n-1)}{2}A_i^+(\mathbf{a}(t))\int\limits_{\mathbb{R}^m} y_i^{n-2}P(\mathbf{y},t)d\mathbf{y}\\
\Rightarrow \frac{d}{dt}\mathbb{E}[y_i^n] &= n\mathbb{E}[y_i^{n-1}D_{i}]+\frac{n(n-1)}{2}A_i^+(\mathbf{a}(t))\mathbb{E}[y_i^{n-2}]\label{nD_general_moment_eqns}
\end{align}
Of particular interest are the cases $n=1$ (corresponding to the expected value of $y_i$) and $n=2$ (which can be used along with the expected value to compute the variance of $y_i$). We have:
\begin{align}
\frac{d}{dt}\mathbb{E}[y_i] &= \mathbb{E}[D_{i}]\label{nD_moment_eqn_mean}\\
\frac{d}{dt}\mathbb{E}[y_i^2] &= 2\mathbb{E}[y_iD_{i}] +  A_i^+(\mathbf{a}(t)) = 2\mathrm{Cov}(y_i,D_i) + 2\mathbb{E}[y_i]\mathbb{E}[D_i]+A_i^+(\mathbf{a}(t))\label{nD_moment_eqn_2nd_mom}
% \Rightarrow \frac{d}{dt}\mathrm{Var}(y_i) &= - \mathbb{E}[D_{i}]^2 + 2\mathbb{E}[y_iD_{i}] +  A_i^+(\mathbf{a}(t))\label{nD_moment_eqn_var}
\end{align}
Thus, whether stochastic fluctuations are expected to grow or decay is controlled by $D_i$, a measure of how the growth rate ($b_i - d_i$) changes along the direction of the fluctuation, whereas the spread of the fluctuations (the variance) has contributions from the net turnover rate ($A_i^+ = b_i + d_i$) and the covariance between the fluctuation and $D_i$. In the case of the functional forms given by \eqref{nD_functional_forms_for_replicator}, we have:
\begin{equation}
A_i^-(\mathbf{x}) = w_i(\mathbf{x})x_i + \mu Q_i(\mathbf{x})
\end{equation}
and thus, from \eqref{nD_WNA_taylor_term}, we can calculate the directional derivative $D_i$ as
\begin{align}
D_i &= \sum\limits_{k=1}^{m} y_k\left(\frac{\partial A^{-}_i(\mathbf{x})}{\partial x_k}\bigg{|}_{\mathbf{x}=\mathbf{a}(t)}\right)\\
&= \sum\limits_{k=1}^{m} y_k\left(\frac{\partial}{\partial x_k}\left( w_i(\mathbf{x})x_i + \mu Q_i(\mathbf{x})\right)\bigg{|}_{\mathbf{x}=\mathbf{a}(t)}\right)\\
&= \sum\limits_{k=1}^{m} y_k\left(a_i\frac{\partial w_i}{\partial x_k}\bigg{|}_{\mathbf{x}=\mathbf{a}(t)}\right) + y_iw_i(\mathbf{a}) + \mu\sum\limits_{k=1}^{m} y_k\left(\frac{\partial Q_i}{\partial x_k}(\mathbf{x})\bigg{|}_{\mathbf{x}=\mathbf{a}(t)}\right)\\
&= y_iw_i(\mathbf{a}) + a_iD_{\mathbf{y}}(w_i(\mathbf{a})) + \mu D_{\mathbf{y}}(Q_i(\mathbf{a}))\label{nD_WNA_directional_derivative_for_replicator_eqns}
\end{align}
Using this in \eqref{nD_moment_eqn_mean}, we see that the expected change of a fluctuation in the density of type $i$ individuals evolves as:
\begin{equation}
\label{nD_moment_eqn_mean_replicator}
\frac{d}{dt}\mathbb{E}[y_i] = \underbrace{w_i(\mathbf{a})\mathbb{E}[y_i]}_{\substack{\text{Current fitness of type $i$} \\ \text{at deterministic trajectory $\mathbf{a}$} \\ \text{(scaled by expected density $\mathbb{E}[y_i]$)}}} + \underbrace{a_i\mathbb{E}[D_{\mathbf{y}}(w_i(\mathbf{a}))]}_{\substack{\text{Expected change in fitness} \\ \text{ of type $i$ in going from $\mathbf{a}$ to $\mathbf{y}$} \\ \text{(scaled by deterministic density $a_i$)}}} + \underbrace{\mu\mathbb{E}[D_{\mathbf{y}}(Q_i(\mathbf{a}))]}_{\substack{\text{Expected effect of} \\ \text{mutations}}}
\end{equation}

